---
title: "HW6"
output: html_document
---

```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE)
suppressWarnings(library(DALEX, quietly = T))
suppressWarnings(library(xgboost, quietly=TRUE))
suppressWarnings(library(randomForest, quietly = TRUE))
suppressWarnings(library(ggplot2, quietly = T))

```

### Confusion table for xgboost

```{r point_0, echo=FALSE}
# 0. For the selected data set, train at least one tree-based ensemble model, e.g. random forest, gbdt, xgboost.

setwd("C:/Projects/MIMUW/xai/fork/eXplainableMachineLearning-2024/Homeworks/HW5")

dataset = "SpeedDating.csv"
repo_url = "https://raw.githubusercontent.com/adrianstando/imbalanced-benchmarking-set/main/datasets/"

if(!file.exists(dataset)){
  data <- read.csv(paste0(repo_url, dataset), row.names="X")
  write.csv(x=data, file=dataset)
}else{
  data <- read.csv(dataset, row.names="X")
}

### train xgb
set.seed(42)

data <- data[sample(1:NROW(data)),]
train <- data[1:800,]
test <- data[801:NROW(data),]

# convert to DMatix object

dtrain <- xgb.DMatrix(data = as.matrix(train)[,1:(NCOL(train)-1)], 
                      label = train$TARGET)
dtest <- xgb.DMatrix(data = as.matrix(test)[,1:(NCOL(test)-1)], 
                     label = test$TARGET)

params <- list(
  booster = "gbtree",
  objective = "binary:logistic",
  eta = 0.3,
  max_depth = 6
)

xgb_model <- xgb.train(
  params = params,
  data = dtrain,
  nrounds = 100,
  verbose=2
)

# Calculate the predictions for some selected observations.
predictions <- predict(xgb_model, dtest)

predictions_binary <- ifelse(predictions > 0.5, 1, 0)
table(predictions_binary, test$TARGET)
# df_predictions <- data.frame(id=rownames(test), predictions, predictions_binary, TARGET=test$TARGET)
# 
# print(head(df_predictions, 2))
```

### 1 Permutation-based Variable Importance for xgboost

```{r, vip_xgb, echo=FALSE}
explainer <- DALEX::explain(model = xgb_model,
                     data = as.matrix(train)[,1:(NCOL(train)-1)],
                     label = "XGBoost Model",
                     y = train$TARGET, 
                     verbose=0,
                     type="classification")

importance_scores_xgb <- DALEX::model_parts(explainer)
plot(importance_scores_xgb)

```

### Four different random forest models

```{r, rfs, echo=FALSE}
# 2.Train three more candidate models (different variable transformations, different model architectures, hyperparameters) and compare their rankings of important features using PVI. What are the differences? Why?

rf_mtry2 <- randomForest(formula=as.factor(TARGET)~., 
                         data=train, 
                         ntree=1000, 
                         mtry=2)

rf_mtry3 <- randomForest(formula=as.factor(TARGET)~., 
                         data=train, 
                         ntree=1000, 
                         mtry=3) # instead of 2

rf_nodesize2 <- randomForest(formula=as.factor(TARGET)~., 
                             data=train, 
                             ntree=1000, 
                             mtry=3, 
                             nodesize=2) # default is unrestricted

rf_mtry1 <- randomForest(formula=as.factor(TARGET)~., 
                             data=train, 
                             ntree=1000, 
                             mtry=1, nodesize=5) # default is unrestricted

rfs <- list(rf_mtry2, rf_mtry3, rf_nodesize2, rf_mtry1)

oob_err_mtry2 <- rf_mtry2$err.rate[nrow(rf_mtry2$err.rate), "OOB"]
names(oob_err_mtry2) <- "oob_err model mtry=2"

oob_err_mtry3 <- rf_mtry3$err.rate[nrow(rf_mtry3$err.rate), "OOB"]
names(oob_err_mtry3) = "oob_err model mtry=3"

oob_err_nodesize2 <- rf_nodesize2$err.rate[nrow(rf_nodesize2$err.rate), "OOB"]
names(oob_err_nodesize2) = "oob_err model mtry=3 nodesize=2"

oob_err_mtry1 <- rf_mtry1$err.rate[nrow(rf_mtry1$err.rate), "OOB"]
names(oob_err_mtry1) = "oob_err model mtry=1"

results_rf <- c(oob_err_mtry2, oob_err_mtry3, oob_err_nodesize2, oob_err_mtry1)
```

### OOB errors for four RF models

* *mtry* - number of variables to choose from each split in node (same variables for all nodes in given tree), larger increase overfitting as variable for split is choosen in greedy way, defaults to sqrt of number of columns for classification

* *nodesize* - max number of obs. per final node, larger increase regularization, defaults to 5 for classification

```{r oob_rf}
print(data.frame(results_rf ))
```

### Four explainers fo RFs - permutation based errors

```{r VPI_for_rfs}
create_explainer <- function(model, label){
  DALEX::explain(model = model,
  data = as.matrix(train)[,1:(NCOL(train)-1)],
  label = label,
  y = train$TARGET, 
  verbose=0)
  }

rf_model_labels <- c("model mtry=2", "model mtry=4",
                       "model mtry=3 nodesize=2", 
                       "model mtry=1 nodesize=5")

explainers <- mapply(function(x, y) create_explainer(x, y), 
                     rfs, 
                     rf_model_labels,
                     SIMPLIFY=FALSE
                   )

rf_importance_scores <- lapply(
  explainers, 
  function(model)DALEX::model_parts(model)
)
```

```{r rf_VPI_plots}
lapply(rf_importance_scores, plot)
```

* Increasing mtry from 2 to 4 increases 1-AUC but does not change order of variables importance because more important variable are more often in use
* Lowering mtry to 3 from 4 and setting nodesize to 2, which is lower then default 5, keeps plot unchanged, it seems that lowering nodesize conteract to lowering mtry - order of most important variables is unchanged
* On last RF plot mtry=1 competition between variables during spilts in nodes
is switched-off it lowes dependence of final model from most important variable
as they are less frequently choosen, the result is increase in importance of previously less important variable
* Below PVI for XGBoost, similar to RF with mtry=3 despite the fact that trees in xgboost are dependent (`n`-th tree depends on output of `n-1` tree) I was supposing that weak predictors would have higher overall importance because they could by chance randomly incorporated in previous tree then by permuting them we would impact consecutive trees - but it is not the case

```{r xgb_PVI, echo=FALSE}
plot(importance_scores_xgb)

```
### Gini impurity based results

* Overall order of variable is the same as for PVI
* Again top 5 most important variables are the same
* Weak predictors have assigned higher values of importance bacause Gini based importance is correlated with frequency of variable usage in model then the lower *mtry* value is the higher are importance scores for weak predictors (this effect is only visible for mtry=1 model versus the rest as max importance is visibly lower in this)
* "I do not see the influence of the number of levels in variables on the results of variable importance based on Gini impurity (table at the bottom), almost all variables have about 20 levels (*interests_correlate* with 132 levels is continuous variable)


```{r xgb_feature_importances_, echo=FALSE}
importance_scores <- xgb.importance(model = xgb_model)

ggplot(data = importance_scores) +
  geom_bar(aes(x = Gain, y = reorder(Feature, Gain)), stat = "identity") +
  labs(title = "Feature Importance (Gini impurity) for XGBoost") +
  theme_minimal()


```


```{r gini_var_ipm}
explainers <- mapply(function(x, label){ 
    varImpPlot(x, main=paste0("Random forest ", label))
  }, 
                     rfs, 
                     rf_model_labels,
                     SIMPLIFY=FALSE
                   )

```

### Number of levels in each variable

```{r table}
for(name in names(train)){
  value <- nlevels(factor(train[[name]]))
  print(c(name, value))
}
```

<!-- ```{r train_rf} -->

<!-- rf_mtry2 <- randomForest(formula=as.factor(TARGET)~.,  -->

<!--                          data=train,  -->

<!--                          ntree=1000,  -->

<!--                          mtry=2) -->

<!-- ``` -->

<!-- ```{r calc_vip} -->

<!-- # 1.Calculate Permutation-based Variable Importance for the selected model. -->

<!-- importance_matrix <- xgb.importance( -->

<!--   feature_names = colnames(train[,1:(NCOL(train)-1)]), -->

<!--   model = xgb_model) -->

<!-- # # Wyświetlenie ważności cech -->

<!-- # print(importance_matrix) -->

<!-- xgb.plot.importance(importance_matrix, main="xgb model Permutation Variable Importance") -->

<!-- randomForest::varImpPlot(rf_mtry2) -->

<!-- ``` -->

<!-- ```{r point_3} -->

<!-- # 3. For the tree-based model from (1), compare PVI with: -->

<!-- # A) the traditional feature importance measures for trees: Gini impurity etc.; what is implemented in a given library: see e.g. the feature_importances_ attribute in xgboost and sklearn. -->

<!-- # B) [in Python] SHAP variable importance based on the TreeSHAP algorithm available in the shap package. -->

<!-- ``` -->
